{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping non-directory item: .DS_Store\n",
      "Processing stock: A\n",
      "Merged data saved for stock: A\n",
      "Processing stock: AA\n",
      "Merged data saved for stock: AA\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def merge_stock_data(stock_dir):\n",
    "    \"\"\"Merge all bbo and trade files for a single stock and save the result.\"\"\"\n",
    "    bbo_dir = os.path.join(stock_dir, 'bbo')\n",
    "    trade_dir = os.path.join(stock_dir, 'trade')\n",
    "    \n",
    "    # Ensure subdirectories exist\n",
    "    if not os.path.exists(bbo_dir) or not os.path.exists(trade_dir):\n",
    "        print(f\"Skipping {stock_dir}: Missing 'bbo' or 'trade' subdirectories.\")\n",
    "        return None\n",
    "    \n",
    "    daily_dfs = []\n",
    "    \n",
    "    # Get sorted list of files in both directories\n",
    "    bbo_files = sorted(os.listdir(bbo_dir))\n",
    "    trade_files = sorted(os.listdir(trade_dir))\n",
    "    \n",
    "    # Match files by name (assuming they correspond, e.g., '1.csv', '2.csv')\n",
    "    for bbo_file, trade_file in zip(bbo_files, trade_files):\n",
    "        bbo_path = os.path.join(bbo_dir, bbo_file)\n",
    "        trade_path = os.path.join(trade_dir, trade_file)\n",
    "        \n",
    "        if os.path.isfile(bbo_path) and os.path.isfile(trade_path):\n",
    "            # Load the files\n",
    "            bbo_df = pd.read_parquet(bbo_path)\n",
    "            trade_df = pd.read_parquet(trade_path)\n",
    "            \n",
    "            # Merge on a common key (adjust the key column name as necessary)\n",
    "            merged_df = pd.merge(bbo_df, trade_df, on='xltime', how='inner')  # Replace 'timestamp' if needed\n",
    "\n",
    "            # Check if 'xltime' column exists and transform it to datetime\n",
    "            if 'xltime' in merged_df.columns:\n",
    "                merged_df['datetime'] = pd.to_datetime('1899-12-30') + pd.to_timedelta(merged_df['xltime'], unit='D')\n",
    "            else:\n",
    "                print(\"'xltime' column not found in the dataset.\")\n",
    "\n",
    "            aggregated_df = (\n",
    "                merged_df.groupby(['datetime'])  # Grouping by 'datetime' or any key to identify duplicates\n",
    "                .apply(lambda g: pd.Series({\n",
    "                    'weighted_bid_price': (g['bid-price'] * g['bid-volume']).sum() / g['bid-volume'].sum(),\n",
    "                    'total_bid_quantity': g['bid-volume'].sum(),\n",
    "                    'max_bid_quantity': g['bid-volume'].max(),\n",
    "                    'weighted_ask_price': (g['ask-price'] * g['ask-volume']).sum() / g['ask-volume'].sum(),\n",
    "                    'total_ask_quantity': g['ask-volume'].sum(),\n",
    "                    'max_ask_quantity': g['ask-volume'].max(),\n",
    "                    'total_trade_volume': g['trade-volume'].sum(),\n",
    "\n",
    "\n",
    "                }))\n",
    "                .reset_index()\n",
    "            )\n",
    "            \n",
    "            # Append the merged DataFrame to the list\n",
    "            daily_dfs.append(aggregated_df)\n",
    "        else:\n",
    "            print(f\"Skipping unmatched files: {bbo_file}, {trade_file}\")\n",
    "    \n",
    "    # Concatenate all daily DataFrames into one for the stock\n",
    "    if daily_dfs:\n",
    "        stock_merged_df = pd.concat(daily_dfs, ignore_index=True)\n",
    "        \n",
    "        # Save the merged DataFrame in the stock's folder\n",
    "        output_path = os.path.join(stock_dir, 'merged_data.csv')  # Save as CSV\n",
    "        stock_merged_df.to_csv(output_path, index=False)\n",
    "        print(f\"Merged data saved for stock: {os.path.basename(stock_dir)}\")\n",
    "    else:\n",
    "        print(f\"No data to merge for stock: {os.path.basename(stock_dir)}\")\n",
    "\n",
    "def process_all_stocks(base_dir):\n",
    "    \"\"\"Process all stocks in the base directory.\"\"\"\n",
    "    for stock in sorted(os.listdir(base_dir)):\n",
    "        stock_dir = os.path.join(base_dir, stock)\n",
    "        if os.path.isdir(stock_dir):\n",
    "            print(f\"Processing stock: {stock}\")\n",
    "            merge_stock_data(stock_dir)\n",
    "        else:\n",
    "            print(f\"Skipping non-directory item: {stock}\")\n",
    "\n",
    "# Example usage\n",
    "base_directory = \"/Users/othmaneio/Documents/financial_big_data/data\"  # Replace with the path to your stock directories\n",
    "process_all_stocks(base_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting polars\n",
      "  Downloading polars-1.17.1-cp39-abi3-macosx_10_12_x86_64.whl (33.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 33.0 MB 3.9 kB/s eta 0:00:011     |█████████████████▊              | 18.3 MB 14.0 MB/s eta 0:00:02MB 11.1 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: polars\n",
      "Successfully installed polars-1.17.1\n"
     ]
    }
   ],
   "source": [
    "!pip install polars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping non-directory item: .DS_Store\n",
      "Processing stock: A\n",
      "Merged data saved as CSV for stock: A\n",
      "Processing stock: AA\n",
      "Merged data saved as CSV for stock: AA\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "\n",
    "def process_stock_with_polars(stock_dir):\n",
    "    \"\"\"Merge bbo and trade files for a single stock using Polars and perform aggregations.\"\"\"\n",
    "    bbo_dir = os.path.join(stock_dir, 'bbo')\n",
    "    trade_dir = os.path.join(stock_dir, 'trade')\n",
    "\n",
    "    # Ensure subdirectories exist\n",
    "    if not os.path.exists(bbo_dir) or not os.path.exists(trade_dir):\n",
    "        print(f\"Skipping {stock_dir}: Missing 'bbo' or 'trade' subdirectories.\")\n",
    "        return\n",
    "\n",
    "    # Get sorted list of files in both directories\n",
    "    bbo_files = sorted(os.listdir(bbo_dir))\n",
    "    trade_files = sorted(os.listdir(trade_dir))\n",
    "\n",
    "    lazy_dfs = []\n",
    "\n",
    "    for bbo_file, trade_file in zip(bbo_files, trade_files):\n",
    "        bbo_path = os.path.join(bbo_dir, bbo_file)\n",
    "        trade_path = os.path.join(trade_dir, trade_file)\n",
    "\n",
    "        if os.path.isfile(bbo_path) and os.path.isfile(trade_path):\n",
    "            # Load the Parquet files as Polars lazy DataFrames\n",
    "            bbo_df = pl.read_parquet(bbo_path).lazy()\n",
    "            trade_df = pl.read_parquet(trade_path).lazy()\n",
    "\n",
    "\n",
    "            # Merge on xltime\n",
    "            merged_df = bbo_df.join(trade_df, on=\"xltime\", how=\"inner\")\n",
    "\n",
    "            # Ensure consistent types\n",
    "            merged_df = merged_df.with_columns([\n",
    "                pl.col(\"xltime\").cast(pl.Float64),         # Ensure `xltime` is Float64\n",
    "                pl.col(\"bid-price\").cast(pl.Float64),      # Ensure `bid-price` is Float64\n",
    "                pl.col(\"bid-volume\").cast(pl.Float64),     # Ensure `bid-volume` is Float64\n",
    "                pl.col(\"ask-price\").cast(pl.Float64),      # Ensure `ask-price` is Float64\n",
    "                pl.col(\"ask-volume\").cast(pl.Float64),     # Ensure `ask-volume` is Float64\n",
    "                pl.col(\"trade-price\").cast(pl.Float64),    # Ensure `trade-price` is Float64\n",
    "                pl.col(\"trade-volume\").cast(pl.Float64),   # Ensure `trade-volume` is Float64\n",
    "            ])\n",
    "\n",
    "            # Perform initial aggregation by `xltime`\n",
    "            aggregated_df = (\n",
    "                merged_df\n",
    "                .group_by(\"xltime\")\n",
    "                .agg([\n",
    "                    ((pl.col(\"bid-price\") * pl.col(\"bid-volume\")).sum() / pl.col(\"bid-volume\").sum()).alias(\"weighted_bid_price\"),\n",
    "                    pl.col(\"bid-volume\").sum().alias(\"total_bid_quantity\"),\n",
    "                    pl.col(\"bid-volume\").max().alias(\"max_bid_quantity\"),\n",
    "                    ((pl.col(\"ask-price\") * pl.col(\"ask-volume\")).sum() / pl.col(\"ask-volume\").sum()).alias(\"weighted_ask_price\"),\n",
    "                    pl.col(\"ask-volume\").sum().alias(\"total_ask_quantity\"),\n",
    "                    pl.col(\"ask-volume\").max().alias(\"max_ask_quantity\"),\n",
    "                    pl.col(\"trade-volume\").sum().alias(\"total_trade_volume\"),\n",
    "                    ((pl.col(\"trade-price\") * pl.col(\"trade-volume\")).sum() / pl.col(\"trade-volume\").sum()).alias(\"weighted_trade_price\")\n",
    "                ])\n",
    "            )\n",
    "\n",
    "            # Convert `xltime` to `datetime`\n",
    "            excel_base_date = pl.datetime(1899, 12, 30)  # Excel starts counting from 1900-01-01, but Polars needs 1899-12-30\n",
    "            aggregated_df = aggregated_df.with_columns(\n",
    "                (pl.col(\"xltime\") * pl.duration(days=1) + excel_base_date).alias(\"datetime\")\n",
    "            )\n",
    "            aggregated_df = aggregated_df.with_columns(pl.col(\"datetime\").dt.convert_time_zone(\"America/New_York\"))\n",
    "\n",
    "            # Append the lazy DataFrame for processing\n",
    "            lazy_dfs.append(aggregated_df)\n",
    "        else:\n",
    "            print(f\"Skipping unmatched files: {bbo_file}, {trade_file}\")\n",
    "\n",
    "    if lazy_dfs:\n",
    "        # Combine all daily lazy DataFrames into one\n",
    "        all_data = pl.concat(lazy_dfs)\n",
    "\n",
    "        # Collect and write to CSV\n",
    "        final_df = all_data.collect()\n",
    "        output_path = os.path.join(stock_dir, \"merged_data.csv\")\n",
    "        final_df.write_csv(output_path)\n",
    "        print(f\"Merged data saved as CSV for stock: {os.path.basename(stock_dir)}\")\n",
    "    else:\n",
    "        print(f\"No data to merge for stock: {os.path.basename(stock_dir)}\")\n",
    "\n",
    "def process_all_stocks_with_polars(base_dir):\n",
    "    \"\"\"Process all stocks in the base directory using Polars.\"\"\"\n",
    "    for stock in sorted(os.listdir(base_dir)):\n",
    "        stock_dir = os.path.join(base_dir, stock)\n",
    "        if os.path.isdir(stock_dir):\n",
    "            print(f\"Processing stock: {stock}\")\n",
    "            process_stock_with_polars(stock_dir)\n",
    "        else:\n",
    "            print(f\"Skipping non-directory item: {stock}\")\n",
    "\n",
    "# Example usage\n",
    "base_directory = \"/Users/othmaneio/Documents/financial_big_data/data\"  # Replace with the path to your stock directories\n",
    "process_all_stocks_with_polars(base_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_A_data = pd.read_csv(\"/Users/othmaneio/Documents/financial_big_data/data/A/merged_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xltime</th>\n",
       "      <th>weighted_bid_price</th>\n",
       "      <th>total_bid_quantity</th>\n",
       "      <th>max_bid_quantity</th>\n",
       "      <th>weighted_ask_price</th>\n",
       "      <th>total_ask_quantity</th>\n",
       "      <th>max_ask_quantity</th>\n",
       "      <th>total_trade_volume</th>\n",
       "      <th>weighted_trade_price</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40301.773480</td>\n",
       "      <td>36.75000</td>\n",
       "      <td>102.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>36.760000</td>\n",
       "      <td>108.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>36.76</td>\n",
       "      <td>2010-05-03T14:33:48.651000-0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40301.572999</td>\n",
       "      <td>36.58000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>36.590000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>36.59</td>\n",
       "      <td>2010-05-03T09:45:07.117000-0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40301.646308</td>\n",
       "      <td>36.52000</td>\n",
       "      <td>76.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>36.530000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>36.53</td>\n",
       "      <td>2010-05-03T11:30:40.994000-0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40301.575024</td>\n",
       "      <td>36.62000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>36.630000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>36.62</td>\n",
       "      <td>2010-05-03T09:48:02.040000-0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40301.740158</td>\n",
       "      <td>36.85000</td>\n",
       "      <td>72.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.860000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>36.86</td>\n",
       "      <td>2010-05-03T13:45:49.634000-0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132080</th>\n",
       "      <td>40326.632116</td>\n",
       "      <td>32.45000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>32.460000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>32.45</td>\n",
       "      <td>2010-05-28T11:10:14.850000-0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132081</th>\n",
       "      <td>40326.830700</td>\n",
       "      <td>32.46000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>32.460000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>32.46</td>\n",
       "      <td>2010-05-28T15:56:12.464000-0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132082</th>\n",
       "      <td>40326.823511</td>\n",
       "      <td>32.50020</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>32.514316</td>\n",
       "      <td>380.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>32.51</td>\n",
       "      <td>2010-05-28T15:45:51.328000-0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132083</th>\n",
       "      <td>40326.659751</td>\n",
       "      <td>32.51375</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32.529677</td>\n",
       "      <td>62.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>32.52</td>\n",
       "      <td>2010-05-28T11:50:02.492000-0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132084</th>\n",
       "      <td>40326.697561</td>\n",
       "      <td>32.28000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32.300000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>32.28</td>\n",
       "      <td>2010-05-28T12:44:29.262000-0400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132085 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              xltime  weighted_bid_price  total_bid_quantity  \\\n",
       "0       40301.773480            36.75000               102.0   \n",
       "1       40301.572999            36.58000                10.0   \n",
       "2       40301.646308            36.52000                76.0   \n",
       "3       40301.575024            36.62000                30.0   \n",
       "4       40301.740158            36.85000                72.0   \n",
       "...              ...                 ...                 ...   \n",
       "132080  40326.632116            32.45000                 8.0   \n",
       "132081  40326.830700            32.46000                 4.0   \n",
       "132082  40326.823511            32.50020               100.0   \n",
       "132083  40326.659751            32.51375                 8.0   \n",
       "132084  40326.697561            32.28000                 2.0   \n",
       "\n",
       "        max_bid_quantity  weighted_ask_price  total_ask_quantity  \\\n",
       "0                    9.0           36.760000               108.0   \n",
       "1                   10.0           36.590000                10.0   \n",
       "2                   19.0           36.530000                12.0   \n",
       "3                   15.0           36.630000                 6.0   \n",
       "4                   18.0           36.860000                22.0   \n",
       "...                  ...                 ...                 ...   \n",
       "132080               4.0           32.460000                 4.0   \n",
       "132081               4.0           32.460000                 2.0   \n",
       "132082              10.0           32.514316               380.0   \n",
       "132083               3.0           32.529677                62.0   \n",
       "132084               2.0           32.300000                 5.0   \n",
       "\n",
       "        max_ask_quantity  total_trade_volume  weighted_trade_price  \\\n",
       "0                    9.0              1400.0                 36.76   \n",
       "1                   10.0               400.0                 36.59   \n",
       "2                    3.0               500.0                 36.53   \n",
       "3                    3.0               200.0                 36.62   \n",
       "4                    7.0               400.0                 36.86   \n",
       "...                  ...                 ...                   ...   \n",
       "132080               2.0               200.0                 32.45   \n",
       "132081               2.0               100.0                 32.46   \n",
       "132082              41.0              1800.0                 32.51   \n",
       "132083              15.0               500.0                 32.52   \n",
       "132084               5.0               100.0                 32.28   \n",
       "\n",
       "                               datetime  \n",
       "0       2010-05-03T14:33:48.651000-0400  \n",
       "1       2010-05-03T09:45:07.117000-0400  \n",
       "2       2010-05-03T11:30:40.994000-0400  \n",
       "3       2010-05-03T09:48:02.040000-0400  \n",
       "4       2010-05-03T13:45:49.634000-0400  \n",
       "...                                 ...  \n",
       "132080  2010-05-28T11:10:14.850000-0400  \n",
       "132081  2010-05-28T15:56:12.464000-0400  \n",
       "132082  2010-05-28T15:45:51.328000-0400  \n",
       "132083  2010-05-28T11:50:02.492000-0400  \n",
       "132084  2010-05-28T12:44:29.262000-0400  \n",
       "\n",
       "[132085 rows x 10 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_A_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with duplicate 'xltime': 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def count_duplicate_xltime_rows(df):\n",
    "    \"\"\"Count the number of rows that have a duplicate 'xltime' value.\"\"\"\n",
    "    if 'xltime' not in df.columns:\n",
    "        raise ValueError(\"'xltime' column not found in the DataFrame.\")\n",
    "    \n",
    "    # Find duplicated rows based on 'xltime'\n",
    "    duplicated_count = df['xltime'].duplicated(keep=False).sum()\n",
    "    \n",
    "    return duplicated_count\n",
    "\n",
    "# Example usage with your DataFrame\n",
    "duplicated_rows_count = count_duplicate_xltime_rows(merged_A_data)  # Replace 'your_dataframe' with your actual DataFrame\n",
    "print(f\"Number of rows with duplicate 'xltime': {duplicated_rows_count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaexam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
